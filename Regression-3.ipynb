{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Assignment-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Ridge Regression is a type of linear regression that includes a penalty term to reduce model \n",
    "# complexity and prevent overfitting. It is a regularized form of Ordinary Least Squares (OLS) regression \n",
    "# that shrinks the regression coefficients by imposing an 𝐿2 penalty on their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the assumptions of Ridge Regression?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# 1. Linearity:\n",
    "# The relationship between the independent variables (𝑋) and the dependent variable (𝑦) is assumed to be linear.\n",
    "# If the relationship is non-linear, transformation techniques (e.g. polynomial features) may be required.\n",
    "\n",
    "# 2. Independence (No Autocorrelation)\n",
    "# Observations should be independent of each other.\n",
    "# In time series data, this assumption is often violated due to autocorrelation.\n",
    "\n",
    "# 3. No Perfect Multicollinearity\n",
    "# Unlike OLS, Ridge Regression can handle multicollinearity, but it still assumes that predictors are not \n",
    "# perfectly collinear.\n",
    "# If predictors are highly correlated, Ridge shrinks the coefficients to reduce overfitting.\n",
    "\n",
    "# 4. Feature Scaling Matters\n",
    "# Since Ridge Regression applies an L2 penalty, it is sensitive to the scale of variables.\n",
    "# Features should be standardized (zero mean, unit variance) using techniques like StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The tuning parameter λ controls the strength of regularization in Ridge Regression. Choosing an optimal λ is crucial\n",
    "# for balancing bias and variance.\n",
    "\n",
    "# By using Cross-Validation techniques like GridSearchCV or RidgeCV, we can find the optimum value of λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Ridge Regression is not directly used for feature selection because it shrinks coefficients but does not force\n",
    "# them to zero. However, it can still indirectly aid feature selection in certain ways.\n",
    "\n",
    "# 1: Using Ridge as a Preprocessing Step for Feature Selection\n",
    "# Train a Ridge model and identify low-importance features.\n",
    "# Remove these features and retrain a simpler model.\n",
    "\n",
    "# 2: Hybrid Approach: Ridge + Recursive Feature Elimination (RFE)\n",
    "# Ridge Regression can be combined with Recursive Feature Elimination (RFE) to select features.\n",
    "# RFE iteratively removes the least important features and retrains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Multicollinearity occurs when independent variables (predictors) are highly correlated with each other.\n",
    "\n",
    "# Ridge Regression reduces the impact of multicollinearity by adding an L2 penalty to the regression coefficients.\n",
    "\n",
    "\n",
    "# 1. Shrinks Coefficients to Stabilize the Model\n",
    "# OLS regression computes coefficients using:\n",
    "# β = (X^T * X)^-1 * X^T * y\n",
    "# If (X^T * X) is nearly singular (due to high correlation), the inverse becomes unstable.\n",
    "\n",
    "# Ridge modifies the equation by adding λI to improve stability:\n",
    "# β = (X^T * X + λI)^-1 * X^T * y\n",
    "# Here λI ensures that (X^T * X) remains invertible, even with multicollinearity.\n",
    "\n",
    "# 2. Distributes Weights Among Correlated Features\n",
    "# OLS assigns arbitrary large weights to correlated variables.\n",
    "# Ridge evenly distributes the impact across correlated features, preventing over-reliance on one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Yes, Ridge Regression can handle both categorical and continuous independent variables. However, \n",
    "# categorical variables need to be properly encoded before being used in the model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\n",
    "# # Ans:\n",
    "\n",
    "# # Interpreting the coefficients in Ridge Regression is similar to interpreting coefficients in ordinary least \n",
    "# # squares (OLS) regression, but with a few key differences due to the regularization aspect of Ridge Regression.\n",
    "\n",
    "# General Interpretation\n",
    "\n",
    "# Magnitude: The magnitude of a coefficient indicates the strength of the relationship between the predictor \n",
    "# variable and the response variable. Larger magnitudes (further from zero) suggest a stronger influence.  \n",
    " \n",
    "# Sign: The sign of a coefficient indicates the direction of the relationship:\n",
    "# Positive: An increase in the predictor variable is associated with an increase in the response variable.\n",
    "# Negative: An increase in the predictor variable is associated with a decrease in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Yes, Ridge Regression can be used for time-series analysis, but with certain considerations since it does not \n",
    "# inherently handle temporal dependencies like ARIMA or LSTMs. However, it can be effective for forecasting when \n",
    "# combined with feature engineering techniques.\n",
    "\n",
    "# Feature Engineering Techniques:\n",
    "# 1: Lag features and rolling statistics are used.\n",
    "# 2: Regularization controls overfitting in multicollinear time-series data.\n",
    "# 3: TimeSeriesSplit cross-validation is applied for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
